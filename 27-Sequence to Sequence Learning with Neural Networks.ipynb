{
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t8xiE8L9RXK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'apprentissage séquence-à-séquence (Seq2Seq) est un modèle de traitement automatique des langues qui a été introduit pour la première fois en 2014 dans le papier intitulé [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf) de Ilya Sutskever, Oriol Vinyals et Quoc V. Le. Ce papier a marqué une révolution dans le domaine de la compréhension automatique des langues en introduisant l'utilisation de réseaux de neurones pour l'apprentissage séquence-à-séquence.\n",
        "\n",
        "Avant l'arrivée de ce papier, les modèles pour le traitement automatique des langues étaient généralement basés sur des méthodes statistiques et de la logique floue. Cependant, ces méthodes ont montré leurs limites pour traiter des tâches complexes telles que la traduction automatique. Le papier de Sutskever et al. a proposé une nouvelle approche basée sur les réseaux de neurones qui a permis de surmonter ces limites et de obtenir des résultats très prometteurs dans des tâches telles que la traduction automatique."
      ],
      "metadata": {
        "id": "r_8E7mygUYJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Modèle Seq2Seq"
      ],
      "metadata": {
        "id": "72LPAQ0iUcUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Les modèles de séquence à séquence (seq2seq) les plus courants sont les modèles encodeur-décodeur, qui utilisent généralement un réseau neuronal récurrent (RNN) pour encoder la phrase source (entrée) en un seul vecteur. Dans ce notebook, nous ferons référence à ce vecteur unique en tant que vecteur de contexte. Nous pouvons considérer le vecteur de contexte comme étant une représentation abstraite de la phrase d'entrée entière. Ce vecteur est ensuite décodé par un second RNN qui apprend à générer la phrase cible (sortie) mot par mot.\n",
        "![seq2seq.jpg](data:image/jpeg;base64,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)\n",
        "\n",
        "L'image ci-dessus montre un exemple de traduction. L'entrée, \"I am a student\" est passé à l'encoder (LSTM). Notons que cette image est simplifiée, en effet avant de passer la phrase à l'encoder, on la passe d'abord pas la couche *embedding*, qui a pour rôle de convertir les mots en entrée en un vecteur de caractéristiques numériques. \n",
        "\n",
        "A chaque pas de temps, l'entrée de l'encoder est donc l'embedding du mot actuel qu'on va le noter $e(x_t)$, ainsi que le hidden state du pas de temps précédent, $h_{t-1}$, et l'encoder LSTM donne en sortie un nouveau hidden state $h_{t}$. On peut voir le hidden state comme un vecteur qui représente la phrase. \n",
        "\n",
        "Une fois le dernier mot de la phrase a été passé au LSTM à travers la couche d'*embedding*,  on utilise le dernier hidden state, comme le vecteur de contexte. Ce vecteur représente donc la phrase entière.\n",
        "\n",
        "Une fois qu'on a notre vecteur de contexte, on peut commencer à le décoder pour avoir notre phrase de sortie \"je suis étudiant\", notons qu'on ajoute `<sos>` et `<eos>` pour préciser le début et la fin de la phrase. A chaque pas de temps, l'entrée du décodeur est l'embedding du mot  qu'on va noter cette fois $d(y_t)$, ainsi que le hidden state du pas de temps précédent noté cette fois $s_{t-1}$, on initialise donc le hidden state du décodeur, $s_0$ par le vecteur contexte obtenu précédemment.\n",
        "\n",
        "Dans le décodeur, on a besoin d'aller du hidden state à un certain mot, donc à chaque pas de temps, on va passer le hidden state $s_t$ à une couche linéaire afin de prédire ce qu'on pense être le prochain mot dans la phrase, $\\hat{y}_t$.\n",
        "\n",
        "$$\\hat{y}_t = f(s_t)$$\n",
        "\n",
        "Les mots dans le décodeurs sont générés un par un. On utilise `<sos>` pour la première entrée du décodeur, pour les entrées suivantes, on peut utiliser le dernier mot prédit par le décodeur $\\hat{y}_{t-1}$, comme on peut utiliser le vrai mot ${y}_{t-1}$, ce qui s'appelle le teacher forcing, pour plus d'info cliquez [ici](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/). \n",
        "\n",
        "Notons que lors de l'apprentissage du modèle, on connaît le nombre exacte du mot qu'on cherche à prédire, donc on arrête de générer des mots lorsqu'on atteint le nombre voulu, cependant en cas réel, on continue de générer des mots jusqu'on génère `<sos>` ou bien après un certain nombre de mot soient générer.\n",
        "\n",
        "Une fois qu'on a prédit notre phrase, $\\hat{Y} = \\{ \\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_T \\}$, on la compare à notre vraie phrase $Y = \\{ y_1, y_2, ..., y_T \\}$, afin de calculer la fonction loss. On utilise donc cette fonction pour mettre à jour tous les paramèteres du modèle.\n"
      ],
      "metadata": {
        "id": "6MAKI_xiUCmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Préparation des données"
      ],
      "metadata": {
        "id": "e_nAjlnnl8Ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va utiliser la base de donnée de traduction WMT'14 utilisé dans le papier de recherche.\n",
        "\n",
        " On exécute la cellule suivante afin d'avoir les données :"
      ],
      "metadata": {
        "id": "4ZB59nhcn5uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.statmt.org/europarl/v7/fr-en.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbR4SFUdPqQo",
        "outputId": "348df5bf-676b-4424-df2f-b027c64c8a4f",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:31:51.986276Z",
          "iopub.execute_input": "2023-01-29T14:31:51.986762Z",
          "iopub.status.idle": "2023-01-29T14:33:22.664535Z",
          "shell.execute_reply.started": "2023-01-29T14:31:51.986679Z",
          "shell.execute_reply": "2023-01-29T14:33:22.663338Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-30 13:41:02--  https://www.statmt.org/europarl/v7/fr-en.tgz\n",
            "Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\n",
            "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 202718517 (193M) [application/x-gzip]\n",
            "Saving to: ‘fr-en.tgz’\n",
            "\n",
            "fr-en.tgz           100%[===================>] 193.33M  2.20MB/s    in 87s     \n",
            "\n",
            "2023-01-30 13:42:29 (2.21 MB/s) - ‘fr-en.tgz’ saved [202718517/202718517]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Après on va extraire les deux fichiers contenues dans le dossier *fr-en.tgz*"
      ],
      "metadata": {
        "id": "eezogAf9sISq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf fr-en.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEfOge3TPzpb",
        "outputId": "3df50267-630f-4b41-b719-b096a7b4758c",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:33:22.666648Z",
          "iopub.execute_input": "2023-01-29T14:33:22.666967Z",
          "iopub.status.idle": "2023-01-29T14:33:28.899212Z",
          "shell.execute_reply.started": "2023-01-29T14:33:22.666936Z",
          "shell.execute_reply": "2023-01-29T14:33:28.898007Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "europarl-v7.fr-en.en\n",
            "europarl-v7.fr-en.fr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   europarl-v7.fr-en.en contient des phrases en anglais\n",
        "*   europarl-v7.fr-en.fr contient des phrases en français\n",
        "\n",
        "On va traiter et combiner les deux fichiers afin de créer notre jeu de données.\n",
        "\n"
      ],
      "metadata": {
        "id": "lvNSbxo2tMFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On commence par importer les bibliothèques utiles :"
      ],
      "metadata": {
        "id": "0zg_L8JDux8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize"
      ],
      "metadata": {
        "id": "AEmHlWesu-dq",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:33:28.900758Z",
          "iopub.execute_input": "2023-01-29T14:33:28.901175Z",
          "iopub.status.idle": "2023-01-29T14:33:28.906265Z",
          "shell.execute_reply.started": "2023-01-29T14:33:28.901134Z",
          "shell.execute_reply": "2023-01-29T14:33:28.905206Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commençons par charger les fichiers de données.\n",
        "\n",
        "Nous pouvons charger chaque fichier sous forme de chaîne de caractère. Étant donné que les fichiers contiennent des caractères Unicode, nous devons spécifier un encodage lors du chargement des fichiers en tant que texte. Dans ce cas, nous utiliserons UTF-8 qui gérera facilement les caractères Unicode dans les deux fichiers.\n",
        "\n",
        "La fonction ci-dessous, nommée load_doc() , chargera un fichier donné et le renverra sous forme de blob de texte.\n",
        "\n"
      ],
      "metadata": {
        "id": "ufK3PqsXyuCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les données en mémoire\n",
        "def load_doc(filename):\n",
        "\t# Ouvrir le fichier en lecture uniquement\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# On lit tout le texte\n",
        "\ttext = file.read()\n",
        "\t# On ferme le fichier\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "metadata": {
        "id": "eTSvEbR7zFVD",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:33:28.909098Z",
          "iopub.execute_input": "2023-01-29T14:33:28.909721Z",
          "iopub.status.idle": "2023-01-29T14:33:28.916684Z",
          "shell.execute_reply.started": "2023-01-29T14:33:28.909685Z",
          "shell.execute_reply": "2023-01-29T14:33:28.915577Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite, nous pouvons diviser le fichier en phrases.\n",
        "\n",
        "Généralement, un énoncé est stocké sur chaque ligne. Nous pouvons les traiter comme des phrases et diviser le fichier par des caractères de nouvelle ligne. La fonction to_sentences() ci-dessous divisera un document chargé."
      ],
      "metadata": {
        "id": "COqUtNphz5z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# on split un document chargé en phrase\n",
        "def to_sentences(doc):\n",
        "\treturn doc.strip().split('\\n')"
      ],
      "metadata": {
        "id": "0QBUuV2W1mdd",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:33:28.918441Z",
          "iopub.execute_input": "2023-01-29T14:33:28.919099Z",
          "iopub.status.idle": "2023-01-29T14:33:28.924382Z",
          "shell.execute_reply.started": "2023-01-29T14:33:28.919044Z",
          "shell.execute_reply": "2023-01-29T14:33:28.923334Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les données nécessitent un minimum de nettoyage avant d'être utilisées pour former un modèle de traduction neuronale.\n",
        "\n",
        "En regardant quelques exemples de texte, un nettoyage minimal du texte peut inclure :\n",
        "\n",
        "* Tokenisation du texte par un espace blanc.\n",
        "* Normalisation de la phrase en minuscule.\n",
        "* Suppression de la ponctuation de chaque mot.\n",
        "* Suppression des caractères non imprimables.\n",
        "* Conversion des caractères français en caractères latins.\n",
        "* Suppression de mots contenant des caractères non alphabétiques.\n",
        "\n",
        "La fonction clean_lines() ci- dessous implémente ces opérations de nettoyage. Quelques notes:\n",
        "\n",
        "* Nous utilisons l'API unicode pour normaliser les caractères unicode, qui convertit les caractères français en équivalents latins.\n",
        "* Nous utilisons une correspondance d'expression régulière pour ne conserver que les caractères des mots imprimables.\n",
        "* Nous utilisons une table de traduction pour traduire les caractères tels quels, mais excluons tous les caractères de ponctuation\n",
        "\n"
      ],
      "metadata": {
        "id": "VCa2RJdh2aXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nettoyage d'une liste de ligne\n",
        "def clean_lines(lines):\n",
        "\tcleaned = list()\n",
        "\t# on prepare regex pour filtrer les caractères\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# on prépare la table de traduction pour enlever les ponctuations\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor line in lines:\n",
        "\t\t# on normalise les caractères unicodes\n",
        "\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\tline = line.decode('UTF-8')\n",
        "\t\t# tokenize avec espace blanc\n",
        "\t\tline = line.split()\n",
        "\t\t# on convertit en miniscule\n",
        "\t\tline = [word.lower() for word in line]\n",
        "\t\t# on enlève la ponctuation de chaque token\n",
        "\t\tline = [word.translate(table) for word in line]\n",
        "\t\t# on enlève les caractères non imprimables de chaque token\n",
        "\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t# on enlève les tokens qui contiennent des nombres\n",
        "\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t# On stock le résultat en string\n",
        "\t\tcleaned.append(' '.join(line))\n",
        "\treturn cleaned"
      ],
      "metadata": {
        "id": "A0z-QwHs6f_i",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:33:28.925931Z",
          "iopub.execute_input": "2023-01-29T14:33:28.926561Z",
          "iopub.status.idle": "2023-01-29T14:33:28.935699Z",
          "shell.execute_reply.started": "2023-01-29T14:33:28.926526Z",
          "shell.execute_reply": "2023-01-29T14:33:28.934653Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une fois normalisées, nous sauvegardons les listes de lignes propres directement au format binaire à l'aide de l'API pickle. Cela accélérera le chargement pour d'autres opérations ultérieures et futures."
      ],
      "metadata": {
        "id": "-6-Oh9BU790p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarder une liste de phrases nettoyés\n",
        "def save_clean_sentences(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)"
      ],
      "metadata": {
        "id": "yyBH9rqW8DR9",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:33:28.937270Z",
          "iopub.execute_input": "2023-01-29T14:33:28.937623Z",
          "iopub.status.idle": "2023-01-29T14:33:28.944629Z",
          "shell.execute_reply.started": "2023-01-29T14:33:28.937590Z",
          "shell.execute_reply": "2023-01-29T14:33:28.943677Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Réutilisant les fonctions développées dans les sections précédentes pour charger les données et les nettoyer"
      ],
      "metadata": {
        "id": "f9M42hks9G6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load English data\n",
        "filename = 'europarl-v7.fr-en.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "sentences = clean_lines(sentences)\n",
        "save_clean_sentences(sentences, 'english.pkl')\n",
        "# spot check\n",
        "for i in range(10):\n",
        "\tprint(sentences[i])\n",
        "\n",
        "# load French data\n",
        "filename = 'europarl-v7.fr-en.fr'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "sentences = clean_lines(sentences)\n",
        "save_clean_sentences(sentences, 'french.pkl')\n",
        "# spot check\n",
        "for i in range(10):\n",
        "\tprint(sentences[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJCVMnwYRpev",
        "outputId": "d79be545-6b21-42b1-b1a1-a4f47f7d5b65",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:33:28.946222Z",
          "iopub.execute_input": "2023-01-29T14:33:28.946726Z",
          "iopub.status.idle": "2023-01-29T14:35:48.535461Z",
          "shell.execute_reply.started": "2023-01-29T14:33:28.946692Z",
          "shell.execute_reply": "2023-01-29T14:35:48.534374Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english.pkl\n",
            "resumption of the session\n",
            "i declare resumed the session of the european parliament adjourned on friday december and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period\n",
            "although as you will have seen the dreaded millennium bug failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful\n",
            "you have requested a debate on this subject in the course of the next few days during this partsession\n",
            "in the meantime i should like to observe a minute s silence as a number of members have requested on behalf of all the victims concerned particularly those of the terrible storms in the various countries of the european union\n",
            "please rise then for this minute s silence\n",
            "the house rose and observed a minute s silence\n",
            "madam president on a point of order\n",
            "you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka\n",
            "one of the people assassinated very recently in sri lanka was mr kumar ponnambalam who had visited the european parliament just a few months ago\n",
            "Saved: french.pkl\n",
            "reprise de la session\n",
            "je declare reprise la session du parlement europeen qui avait ete interrompue le vendredi decembre dernier et je vous renouvelle tous mes vux en esperant que vous avez passe de bonnes vacances\n",
            "comme vous avez pu le constater le grand bogue de lan ne sest pas produit en revanche les citoyens dun certain nombre de nos pays ont ete victimes de catastrophes naturelles qui ont vraiment ete terribles\n",
            "vous avez souhaite un debat a ce sujet dans les prochains jours au cours de cette periode de session\n",
            "en attendant je souhaiterais comme un certain nombre de collegues me lont demande que nous observions une minute de silence pour toutes les victimes des tempetes notamment dans les differents pays de lunion europeenne qui ont ete touches\n",
            "je vous invite a vous lever pour cette minute de silence\n",
            "le parlement debout observe une minute de silence\n",
            "madame la presidente cest une motion de procedure\n",
            "vous avez probablement appris par la presse et par la television que plusieurs attentats a la bombe et crimes ont ete perpetres au sri lanka\n",
            "lune des personnes qui vient detre assassinee au sri lanka est m kumar ponnambalam qui avait rendu visite au parlement europeen il y a quelques mois a peine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons utiliser pickle.load()pour charger les fichiers maintenant enregistrés, puis nous pouvons utiliser DataFrame de pandas afin de créer notre dataset "
      ],
      "metadata": {
        "id": "v9Er24pe-DQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "with open('french.pkl', 'rb') as f:\n",
        "    fr_voc = pickle.load(f)\n",
        "\n",
        "with open('english.pkl', 'rb') as f:\n",
        "    eng_voc = pickle.load(f)\n",
        "    \n",
        "data = pd.DataFrame(zip(eng_voc, fr_voc), columns = ['English', 'French'])\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "EIjwi7ZaTwy0",
        "outputId": "0f25d9a2-d40c-4402-bb8d-46db732664b9",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:35:48.537166Z",
          "iopub.execute_input": "2023-01-29T14:35:48.537734Z",
          "iopub.status.idle": "2023-01-29T14:35:51.473524Z",
          "shell.execute_reply.started": "2023-01-29T14:35:48.537697Z",
          "shell.execute_reply": "2023-01-29T14:35:51.472606Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   English  \\\n",
              "0                                resumption of the session   \n",
              "1        i declare resumed the session of the european ...   \n",
              "2        although as you will have seen the dreaded mil...   \n",
              "3        you have requested a debate on this subject in...   \n",
              "4        in the meantime i should like to observe a min...   \n",
              "...                                                    ...   \n",
              "2007718  i would also like although they are absent to ...   \n",
              "2007719  i am not going to reopen the millennium or not...   \n",
              "2007720                         adjournment of the session   \n",
              "2007721  i declare the session of the european parliame...   \n",
              "2007722                       the sitting was closed at am   \n",
              "\n",
              "                                                    French  \n",
              "0                                    reprise de la session  \n",
              "1        je declare reprise la session du parlement eur...  \n",
              "2        comme vous avez pu le constater le grand bogue...  \n",
              "3        vous avez souhaite un debat a ce sujet dans le...  \n",
              "4        en attendant je souhaiterais comme un certain ...  \n",
              "...                                                    ...  \n",
              "2007718  je me permettrai meme bien quils soient absent...  \n",
              "2007719  je ne rouvrirai pas le debat sur le millenaire...  \n",
              "2007720                         interruption de la session  \n",
              "2007721  je declare interrompue la session du parlement...  \n",
              "2007722                              la seance est levee a  \n",
              "\n",
              "[2007723 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d1f81c6-42ae-463e-aba3-24db28bd97b3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>French</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>resumption of the session</td>\n",
              "      <td>reprise de la session</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i declare resumed the session of the european ...</td>\n",
              "      <td>je declare reprise la session du parlement eur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>although as you will have seen the dreaded mil...</td>\n",
              "      <td>comme vous avez pu le constater le grand bogue...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>you have requested a debate on this subject in...</td>\n",
              "      <td>vous avez souhaite un debat a ce sujet dans le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>in the meantime i should like to observe a min...</td>\n",
              "      <td>en attendant je souhaiterais comme un certain ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007718</th>\n",
              "      <td>i would also like although they are absent to ...</td>\n",
              "      <td>je me permettrai meme bien quils soient absent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007719</th>\n",
              "      <td>i am not going to reopen the millennium or not...</td>\n",
              "      <td>je ne rouvrirai pas le debat sur le millenaire...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007720</th>\n",
              "      <td>adjournment of the session</td>\n",
              "      <td>interruption de la session</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007721</th>\n",
              "      <td>i declare the session of the european parliame...</td>\n",
              "      <td>je declare interrompue la session du parlement...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007722</th>\n",
              "      <td>the sitting was closed at am</td>\n",
              "      <td>la seance est levee a</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2007723 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d1f81c6-42ae-463e-aba3-24db28bd97b3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2d1f81c6-42ae-463e-aba3-24db28bd97b3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2d1f81c6-42ae-463e-aba3-24db28bd97b3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir du dataset crée on va créer le fichier eng-fra.txt qui contient l'ensemble de notre jeu de donnée"
      ],
      "metadata": {
        "id": "USs4FuXV-luI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('eng-fra.txt',index=False)"
      ],
      "metadata": {
        "id": "ZW1shGm2adgO",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:35:51.477654Z",
          "iopub.execute_input": "2023-01-29T14:35:51.477952Z",
          "iopub.status.idle": "2023-01-29T14:36:03.388057Z",
          "shell.execute_reply.started": "2023-01-29T14:35:51.477925Z",
          "shell.execute_reply": "2023-01-29T14:36:03.387050Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va importer les bibliothèques nécessaires pour préparer l'ensemble des données "
      ],
      "metadata": {
        "id": "FAwsfAVZ-6pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "uoNAK7QsTifl",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:03.389487Z",
          "iopub.execute_input": "2023-01-29T14:36:03.389975Z",
          "iopub.status.idle": "2023-01-29T14:36:05.805993Z",
          "shell.execute_reply.started": "2023-01-29T14:36:03.389933Z",
          "shell.execute_reply": "2023-01-29T14:36:05.805008Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On vérifie bien qu'on utilise GPU"
      ],
      "metadata": {
        "id": "5YM5Yrlw_Kfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z10Y_E--zelq",
        "outputId": "c0dfbd1b-b29b-436e-94bf-07bb1f13f451",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:05.807315Z",
          "iopub.execute_input": "2023-01-29T14:36:05.807890Z",
          "iopub.status.idle": "2023-01-29T14:36:05.815179Z",
          "shell.execute_reply.started": "2023-01-29T14:36:05.807849Z",
          "shell.execute_reply": "2023-01-29T14:36:05.814129Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On stock le chemin vers le fichier qui contient notre jeu de données"
      ],
      "metadata": {
        "id": "yLRmI3TG_NCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path=\"eng-fra.txt\""
      ],
      "metadata": {
        "id": "ug_Y4_tCtkBW",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:05.816938Z",
          "iopub.execute_input": "2023-01-29T14:36:05.817847Z",
          "iopub.status.idle": "2023-01-29T14:36:05.823563Z",
          "shell.execute_reply.started": "2023-01-29T14:36:05.817784Z",
          "shell.execute_reply": "2023-01-29T14:36:05.822511Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour traiter notre ensemble de données pour le traducteur, nous pouvons utiliser cette classe Lang pour fournir des fonctionnalités utiles à notre classe de langue, comme **word2index**, **index2word** et **word2count**"
      ],
      "metadata": {
        "id": "nrnSh87kATzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens de début et de fin\n",
        "Start_sentence_token = 1\n",
        "End_sentence_token = 0\n",
        "\n",
        "\n",
        "#Classe qui définit le langage.\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}# Contient l'index de chaque mot dans le dictionnaire\n",
        "        self.word2count = {}# Contient le nombre de chaque mot\n",
        "        self.index2word = {1: \"SOS\", 0: \"EOS\"} # Contient le mot qui correspond à chaque index\n",
        "        self.n_words = 2  # Le nombre de mot total\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "4ggpN2n5BNcU",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:05.825427Z",
          "iopub.execute_input": "2023-01-29T14:36:05.826277Z",
          "iopub.status.idle": "2023-01-29T14:36:05.834927Z",
          "shell.execute_reply.started": "2023-01-29T14:36:05.826239Z",
          "shell.execute_reply": "2023-01-29T14:36:05.833889Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite, la fonction **readLangs** prend en entrée notre csv pour créer des variables input_lang, output_lang et paires que nous utiliserons pour préparer notre jeu de données.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fL0n0_OdBSEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lire les données\n",
        "def readLangs(lang1, lang2):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Lire les données et diviser en phrase\n",
        "    lines = open(data_path, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # Diviser chaque ligne en des paires\n",
        "    pairs = [[s for s in l.split(',')] for l in lines]\n",
        "    # On crée nos instances de langues.\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "ChELNAtLBVL9",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:05.836666Z",
          "iopub.execute_input": "2023-01-29T14:36:05.837530Z",
          "iopub.status.idle": "2023-01-29T14:36:05.844464Z",
          "shell.execute_reply.started": "2023-01-29T14:36:05.837494Z",
          "shell.execute_reply": "2023-01-29T14:36:05.843538Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans la suite, et pour faciliter l'apprentissage, on va garder uniquement les phrases qui ont une taille inférieur à 12 et qui commence par un des prefixes définit ci-dessous."
      ],
      "metadata": {
        "id": "SVFwU52TDH2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 12\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \", \"I don t\", \"Do you\", \"I want\", \"Are you\", \"I have\", \"I think\",\n",
        "       \"I can t\", \"I was\", \"He is\", \"I m not\", \"This is\", \"I just\", \"I didn t\",\n",
        "       \"I am\", \"I thought\", \"I know\", \"Tom is\", \"I had\", \"Did you\", \"Have you\",\n",
        "       \"Can you\", \"He was\", \"You don t\", \"I d like\", \"It was\", \"You should\",\n",
        "       \"Would you\", \"I like\", \"It is\", \"She is\", \"You can t\", \"He has\",\n",
        "       \"What do\", \"If you\", \"I need\", \"No one\", \"You are\", \"You have\",\n",
        "       \"I feel\", \"I really\", \"Why don t\", \"I hope\", \"I will\", \"We have\",\n",
        "       \"You re not\", \"You re very\", \"She was\", \"I love\", \"You must\", \"I can\")\n",
        "eng_prefixes = (map(lambda x: x.lower(), eng_prefixes))\n",
        "eng_prefixes = tuple(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < max_length and len(p[1].split(' ')) < max_length and p[0].startswith(eng_prefixes)\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "PXtDzsBDDcPd",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:05.846311Z",
          "iopub.execute_input": "2023-01-29T14:36:05.847017Z",
          "iopub.status.idle": "2023-01-29T14:36:05.856503Z",
          "shell.execute_reply.started": "2023-01-29T14:36:05.846981Z",
          "shell.execute_reply": "2023-01-29T14:36:05.855539Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enfin, la fonction prepareData rassemble toutes les fonctions d'assistance pour filtrer et finaliser les paires de langues pour l'entraînement de notre modèle."
      ],
      "metadata": {
        "id": "p76WtEjVDhV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Filtered to %s sentence pairs\" % len(pairs))\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Dictionnary size:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "_ajMZZkOZs4O",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:05.857820Z",
          "iopub.execute_input": "2023-01-29T14:36:05.858271Z",
          "iopub.status.idle": "2023-01-29T14:36:05.866002Z",
          "shell.execute_reply.started": "2023-01-29T14:36:05.858235Z",
          "shell.execute_reply": "2023-01-29T14:36:05.864941Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs = prepareData('english', 'francais')"
      ],
      "metadata": {
        "id": "v5lN1NrFZzv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e00a7e-1c99-4953-c2dd-5e7846c31134",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:05.867344Z",
          "iopub.execute_input": "2023-01-29T14:36:05.867807Z",
          "iopub.status.idle": "2023-01-29T14:36:18.659045Z",
          "shell.execute_reply.started": "2023-01-29T14:36:05.867758Z",
          "shell.execute_reply": "2023-01-29T14:36:18.657950Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 2007724 sentence pairs\n",
            "Filtered to 35554 sentence pairs\n",
            "Dictionnary size:\n",
            "english 10945\n",
            "francais 15177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vLGTId4HgHw",
        "outputId": "0d64ef37-204e-4554-e3a2-cd039e0a281c",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:18.660713Z",
          "iopub.execute_input": "2023-01-29T14:36:18.661172Z",
          "iopub.status.idle": "2023-01-29T14:36:18.668732Z",
          "shell.execute_reply.started": "2023-01-29T14:36:18.661132Z",
          "shell.execute_reply": "2023-01-29T14:36:18.667677Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['it is the case of alexander nikitin', 'il sagit du cas dalexandre nikitin']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour entraîner chaque paire nous aurons besoin d'un tenseur d'entrée (index des mots dans la phrase d'entrée) et d'un tenseur cible (index des mots dans la phrase cible). Lors de la création de ces vecteurs, nous ajouterons le token EOS aux deux séquences."
      ],
      "metadata": {
        "id": "5vFZTKgkFGbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(End_sentence_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "# On applique les deux étapes de vectorisation sur un paire anglais/français\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "metadata": {
        "id": "h4AJL_1OZ10v",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:18.670310Z",
          "iopub.execute_input": "2023-01-29T14:36:18.670687Z",
          "iopub.status.idle": "2023-01-29T14:36:18.679750Z",
          "shell.execute_reply.started": "2023-01-29T14:36:18.670649Z",
          "shell.execute_reply": "2023-01-29T14:36:18.678659Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Définition de l'architecture Encoder-Decoder"
      ],
      "metadata": {
        "id": "rALY53MOyFvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'encoder est effectué via un LSTM composé de quatre couche , dont nous allons stocker l'état pour conditionner le ddecoder. Les entrées sont d'abord intégrées dans un espace de dimension fixe, puis codées par lstm. Nous conservons également les états cachés du lstm, car nous devons les transmettre à l'encodeur pour la prochaine itération.\n",
        "\n",
        "Nous créons cela dans le code en créant le module **Encoder**, ce qui nécessite que nous héritions de *torch.nn.Module*. L'encodeur prend les arguments suivants :\n",
        "\n",
        "* **input_size** : la taille des vecteurs qui seront entrés dans l'encodeur.\n",
        "* **emb_dim** : la dimension de la couche d'embedding. Cette couche convertit les vecteurs d'entrée en vecteurs denses de dimensions emb_dim\n",
        "* **hid_dim** : est la dimension des états cachés et cellulaires.\n",
        "* **n_layers** : est le nombre de couches dans le LSTM."
      ],
      "metadata": {
        "id": "VWW6g9WMyL4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On ne va pas parler en détail de la couche d'embedding dans ce notebook. Tout ce que nous devons savoir est qu'il existe une étape avant que les mots (ou plus précisément l'index des mots) ne soient passés à notre LSTM, où les mots sont transformés en vecteur. Pour avoir plus d'informations sur le *word embedding* vous pouvez voir cet [article](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/). "
      ],
      "metadata": {
        "id": "MU_mPuSG0tzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size,emb_dim,hidden_size,n_layers):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.emb_dim=emb_dim\n",
        "        self.n_layers=n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        #Embedding Layer\n",
        "        self.embedding = nn.Embedding(input_size, emb_dim)\n",
        "        #LSTM Layer\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_size,n_layers)\n",
        "        \n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        #Embedding l'input\n",
        "        embedded = self.embedding(input).view(1, 1, -1) #1 seul batch, 1 seul canal\n",
        "        output = embedded\n",
        "        #On fournit le vecteur d'embedding ainsi que le hidden state passé en argument à notre LSTM\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "Pw3eU4udbfKS",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:18.681585Z",
          "iopub.execute_input": "2023-01-29T14:36:18.681972Z",
          "iopub.status.idle": "2023-01-29T14:36:18.692061Z",
          "shell.execute_reply.started": "2023-01-29T14:36:18.681937Z",
          "shell.execute_reply": "2023-01-29T14:36:18.690955Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le décodeur est censé prédire le prochain mot de la phrase cible, connaissant le mot actuel et le vecteur contexte donné par l'encoder. Le vecteur contexte encode la phrase d'entrée et va conditionner toutes les prédictions du décodeur.\n",
        "\n",
        "L'implémentation du décodeur est similaire à celle de l'encodeur, sauf que maintenant on a l'argument **output_size** qui représente la taille du vecteur de sortie. On ajoute aussi une couche linéaire et une fonction d'activation softmax qui vont faire les prédictions."
      ],
      "metadata": {
        "id": "I6YYToad2ohL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size,emb_dim,n_layers):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.output_size=output_size\n",
        "        self.emb_dim=emb_dim\n",
        "        self.n_layers=n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        #Embedding Layer\n",
        "        self.embedding = nn.Embedding(output_size, emb_dim)\n",
        "        #LSTM Layer\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_size,n_layers)\n",
        "        #Linear layer mapping to the output size\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        \n",
        "        #Embedding the input and applying relu\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        \n",
        "        # On fournit le vecteur d'embedding ainsi que le hidden state passé en argument à notre LSTM\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        # Softmax layer (probabilité de chaqud mot)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        \n",
        "        return output, hidden\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lodt8KKq67uH",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:18.693451Z",
          "iopub.execute_input": "2023-01-29T14:36:18.694074Z",
          "iopub.status.idle": "2023-01-29T14:36:18.703727Z",
          "shell.execute_reply.started": "2023-01-29T14:36:18.694038Z",
          "shell.execute_reply": "2023-01-29T14:36:18.703063Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Entraînement du modèle"
      ],
      "metadata": {
        "id": "PFUj_VAr3sUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Premier pas d'entraînement"
      ],
      "metadata": {
        "id": "ioKqW9ku3GpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "D'abord on va écrire une fonction qui définit une seule étape d'entraînement du modèle :"
      ],
      "metadata": {
        "id": "VdPp0DDC3w-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=max_length):\n",
        "    \n",
        "    # On va initialiser le hidden et celle state de l'encodeur aléatoirement\n",
        "    encoder_hidden = torch.randn(4, 1, 1000).to(device)\n",
        "    encoder_cell = torch.randn(4, 1, 1000).to(device)\n",
        "\n",
        "    #On met les gradients à zéero\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    \n",
        "    #Get length of input and target sequences\n",
        "    #On récupère la taille des phrases d'entrée et de sortie\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    loss = 0\n",
        "    \n",
        "    ## ENCODER\n",
        "    #On initialise la sortie de l'encoder à zéro\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    #On passe chaque mot d'entrée à l'encoder. A chaque étape, on récupère la sortie et le hidden/cell states qui forment\n",
        "    #le vecteur contexte. Le vecteur contexte est redonné à l'encoder en entrée pour la prochaine étape\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, (encoder_hidden,encoder_cell) = encoder(input_tensor[ei], (encoder_hidden,encoder_cell))\n",
        "        encoder_outputs[ei] = encoder_output[0, 0] #Le tenseur encoder_output est de taille (1,1,1000) on utilise [0,0] pour extraire 1000 valeurs\n",
        "\n",
        "    ## DECODER\n",
        "    #Pour le décodeur, l'entrée est initialisé avec le token qui indique le début de la phrase.\n",
        "    decoder_input = torch.tensor([[Start_sentence_token]], device=device)\n",
        "\n",
        "    #On initialise les states du décodeur en passant le vecteur contexte donné par l'encodeur\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_cell = encoder_cell \n",
        "    \n",
        "    \n",
        "    #On passe chaque mot cible au décodeur. On garde les hidden et cell states, qui vont être redonnés en entrée au décodeur dans\n",
        "    #l'étape suivante. Notons que le décodeur prend en entrée le mot actuel de la phrase cible.\n",
        "    for di in range(target_length):\n",
        "        decoder_output, (decoder_hidden,decoder_cell) = decoder(decoder_input, (decoder_hidden,decoder_cell))\n",
        "        loss += criterion(decoder_output, target_tensor[di])\n",
        "        decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    #Backward prop\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    #Return loss\n",
        "    return loss.item() / target_length"
      ],
      "metadata": {
        "id": "gVAqWc8E6-VW",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:18.705014Z",
          "iopub.execute_input": "2023-01-29T14:36:18.705525Z",
          "iopub.status.idle": "2023-01-29T14:36:18.716406Z",
          "shell.execute_reply.started": "2023-01-29T14:36:18.705490Z",
          "shell.execute_reply": "2023-01-29T14:36:18.715688Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.2 Affichage de la fonction loss"
      ],
      "metadata": {
        "id": "V5p0w5Xq82w-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Après avoir créer la fonction qui va calculer l'erreur du modèle, la fonction ci-dessous affiche la courbe d'apprentissage"
      ],
      "metadata": {
        "id": "ANUYWFzM86Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n"
      ],
      "metadata": {
        "id": "SVit-mZd6_42",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:18.717548Z",
          "iopub.execute_input": "2023-01-29T14:36:18.718672Z",
          "iopub.status.idle": "2023-01-29T14:36:18.730493Z",
          "shell.execute_reply.started": "2023-01-29T14:36:18.718636Z",
          "shell.execute_reply": "2023-01-29T14:36:18.729852Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.3 Itérations multiples"
      ],
      "metadata": {
        "id": "L6jyb-9g9IN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant on va lancer plusieurs fois l'entraînement du modèle, en mettant à jour les paramètres."
      ],
      "metadata": {
        "id": "mvO-9VtR9S5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    \n",
        "    plot_losses = [] #On va stocker tous les loss pour la courbe d'apprentissage\n",
        "    print_loss_total = 0  # Reset à chaque print_every\n",
        "    plot_loss_total = 0  # Reset à chaque plot_every\n",
        "\n",
        "    #Setup optimizers\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    \n",
        "    #Prepare n_iter training data pour lancer n-iter étapes\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
        "    \n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    \n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        \n",
        "        # Récupérer les tenseurs suivants pour l'entrée et la cible\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "        \n",
        "        # Lancer une étape de l'entraînement\n",
        "        loss = train(input_tensor, target_tensor, encoder,decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        #Tous les quelques pas, nous affichons l'état actuel de l'apprentissage. Nous stockons également les loss pour la courbe\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('(iteration %d %d%%) loss = %.4f' % (iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    #On affiche la courbe d'apprentisage à la fin\n",
        "    showPlot(plot_losses)\n"
      ],
      "metadata": {
        "id": "gsnJfK5b7Bdx",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:18.731631Z",
          "iopub.execute_input": "2023-01-29T14:36:18.732873Z",
          "iopub.status.idle": "2023-01-29T14:36:18.742763Z",
          "shell.execute_reply.started": "2023-01-29T14:36:18.732778Z",
          "shell.execute_reply": "2023-01-29T14:36:18.741821Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.4 Application "
      ],
      "metadata": {
        "id": "PKZI_nz8-awT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va créer maintenant l'encoder et le decoder en utilisant les mêmes modèles que ceux utilisés par les chercheurs, deux LSTM de 4 couches, les hidden state ainsi que la dimension du vecteur embedding sont de 1000."
      ],
      "metadata": {
        "id": "IFIKLKQ7-cMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Entraîner l'encodeur et le décodeur\n",
        "encoder = Encoder(input_lang.n_words,1000,1000,4).to(device)\n",
        "decoder = Decoder(1000, output_lang.n_words,1000,4).to(device)\n",
        "\n",
        "#trainIters(encoder, decoder, 150000, print_every=500,plot_every=100)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dRbNV_LI7DH_",
        "execution": {
          "iopub.status.busy": "2023-01-29T14:36:18.744459Z",
          "iopub.execute_input": "2023-01-29T14:36:18.744933Z",
          "iopub.status.idle": "2023-01-29T17:12:49.050696Z",
          "shell.execute_reply.started": "2023-01-29T14:36:18.744764Z",
          "shell.execute_reply": "2023-01-29T17:12:49.049763Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUvYivj1fIZm",
        "outputId": "c4d38372-949e-4102-abaa-60300953ab9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.Encoder"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va sauvegarder les modèles entraînés :"
      ],
      "metadata": {
        "id": "9SxUYaBy5Lv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(encoder.state_dict(), \"encoder.pt\")\n",
        "torch.save(decoder.state_dict(), \"decoder.pt\")"
      ],
      "metadata": {
        "id": "gj5Vq0AW5Pyu",
        "execution": {
          "iopub.status.busy": "2023-01-29T17:12:49.052058Z",
          "iopub.execute_input": "2023-01-29T17:12:49.052419Z",
          "iopub.status.idle": "2023-01-29T17:12:50.251949Z",
          "shell.execute_reply.started": "2023-01-29T17:12:49.052378Z",
          "shell.execute_reply": "2023-01-29T17:12:50.250967Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pour télécharger les modèles : \n",
        "#L'encoder : https://drive.google.com/file/d/16DyTTAfdvKcGHkgLIxSBZR2yZTZoJcW2/view?usp=sharing\n",
        "#Le decoder : https://drive.google.com/file/d/1rxx_dQpbGCjzfuiLHaaXN4Roo1IG9K8Y/view?usp=sharing"
      ],
      "metadata": {
        "id": "yha2Z1te35F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut upload les modèles déjà entraînés : "
      ],
      "metadata": {
        "id": "BxuoTOFcdx9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "id": "ldCXrQF0d2YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.load_state_dict(torch.load(\"/content/gdrive/MyDrive/Colab Notebooks/colab img/encoder.pt\",map_location=torch.device('cpu')))\n",
        "decoder.load_state_dict(torch.load(\"/content/gdrive/MyDrive/Colab Notebooks/colab img/decoder.pt\",map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "ISyyMY2ad3KB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9607bca8-e028-403e-938b-c48b894ff6c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Inférence "
      ],
      "metadata": {
        "id": "FLv-atMs-xzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La différence entre l'inférence et l'entraînement, c'est que on va continuer de donner en entrée de l'encoder les prédictions déjà faites par le modèle et on ne va s'arrêter que si on prédit le token qui indique la fin de la phrase."
      ],
      "metadata": {
        "id": "3XxJz9ln-zi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(encoder, decoder, sentence, max_length=max_length):\n",
        "    \n",
        "    with torch.no_grad(): #On ne calcule pas les gradients.\n",
        "        \n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        \n",
        "        #On initialise l'encoder et hidden states\n",
        "        encoder_hidden = torch.randn(4, 1, 1000).to(device)\n",
        "        encoder_cell = torch.randn(4, 1, 1000).to(device)\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        ##ENCODER\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, (encoder_hidden,encoder_cell) = encoder(input_tensor[ei],\n",
        "                                                     (encoder_hidden,encoder_cell))\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        #On initialise l'entrée du décodeur avec un token du début\n",
        "        decoder_input = torch.tensor([[Start_sentence_token]], device=device) \n",
        "\n",
        "        #Feed the encoder context vectors to the decoder\n",
        "        #On donne en entrée du décodeur le vecteur contexte de l'encoder\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_cell=encoder_cell\n",
        "\n",
        "        decoded_words = [] #Contient la séquence décodée (traduction)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, (decoder_hidden,decoder_cell) = decoder(decoder_input, (decoder_hidden,decoder_cell))\n",
        "            \n",
        "            topv, topi = decoder_output.data.topk(1) #Renvoie la valeur de l'élement le plus grand ainsi que son index\n",
        "            if topi.item() == End_sentence_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break #On arrête si on prédit un token de fin de phrase\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach() #Utiliser le mot prédit précédemment comme entrée pour l'étape suivante.\n",
        "\n",
        "        return decoded_words"
      ],
      "metadata": {
        "id": "YuoNuHJ_7Gxb",
        "execution": {
          "iopub.status.busy": "2023-01-29T17:12:50.256994Z",
          "iopub.execute_input": "2023-01-29T17:12:50.257290Z",
          "iopub.status.idle": "2023-01-29T17:12:50.268896Z",
          "shell.execute_reply.started": "2023-01-29T17:12:50.257263Z",
          "shell.execute_reply": "2023-01-29T17:12:50.267845Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous définissons une fonction qui évaluera 10 phrases aléatoires de l'ensemble d'entraînement et essaiera de les traduire."
      ],
      "metadata": {
        "id": "zFp_--mOA6eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = inference(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n"
      ],
      "metadata": {
        "id": "-q2jp0Mf7Hbu",
        "execution": {
          "iopub.status.busy": "2023-01-29T17:12:50.271990Z",
          "iopub.execute_input": "2023-01-29T17:12:50.272267Z",
          "iopub.status.idle": "2023-01-29T17:12:50.280066Z",
          "shell.execute_reply.started": "2023-01-29T17:12:50.272241Z",
          "shell.execute_reply": "2023-01-29T17:12:50.279103Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder, decoder)"
      ],
      "metadata": {
        "id": "sAmL-qYC7JJy",
        "execution": {
          "iopub.status.busy": "2023-01-29T17:12:50.281454Z",
          "iopub.execute_input": "2023-01-29T17:12:50.281898Z",
          "iopub.status.idle": "2023-01-29T17:12:50.406950Z",
          "shell.execute_reply.started": "2023-01-29T17:12:50.281859Z",
          "shell.execute_reply": "2023-01-29T17:12:50.405833Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "184b8e10-a7fd-4cf7-e1bd-32da2ff79ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> this is probably because it is not so bad after all\n",
            "= cest probablement parce quil nest pas si mal que cela\n",
            "< cest evidemment faux mais ce ne sont pas la peine de tout\n",
            "\n",
            "> it is wonderful that you are all here again\n",
            "= cest un plaisir de tous vous revoir ici\n",
            "< il est aujourdhui que vous vous etes aujourdhui <EOS>\n",
            "\n",
            "> we have even heard some of them here today\n",
            "= nous en avons entendu quelquesuns ici aujourdhui\n",
            "< nous avons beaucoup entendu aujourdhui aujourdhui <EOS>\n",
            "\n",
            "> this is not such an area\n",
            "= tel ne saurait etre en loccurrence le cas\n",
            "< ce point nest pas nouveau <EOS>\n",
            "\n",
            "> we are all entitled to our opinions\n",
            "= chacun a droit a ses opinions\n",
            "< nous sommes tous responsables a la turquie <EOS>\n",
            "\n",
            "> this is what this report is about\n",
            "= tel est precisement lobjet de ce rapport\n",
            "< cest ce que nous soutenons ce rapport <EOS>\n",
            "\n",
            "> this is also becoming an eu responsibility\n",
            "= cela devient egalement une responsabilite de lue\n",
            "< cest aussi une responsabilite de responsabilite <EOS>\n",
            "\n",
            "> it is not the business of the ep to do so\n",
            "= il nappartient pas au pe de le faire\n",
            "< ce nest plus du tout ce que nous voulons parler <EOS>\n",
            "\n",
            "> it is a very difficult battle\n",
            "= cest une bataille tres difficile\n",
            "< cest un tres tres difficile <EOS>\n",
            "\n",
            "> what do we mean by that\n",
            "= questce quon veut dire par la\n",
            "< que veut dire <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 Conclusion\n"
      ],
      "metadata": {
        "id": "UptvjdlC1Sp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En conclusion, nous avons développé un modèle de traduction utilisant une architecture encoder-decoder avec des LSTM de 4 couches et 1000 neurones pour chaque couche. Les résultats obtenus montrent que le modèle a des difficultés à traduire les phrases avec précision. Il existe plusieurs raisons qui peuvent expliquer ces résultats, tels que la taille limitée de la base de données d'entraînement, les paramètres utilisés dans le modèle, ou encore l'architecture utilisée. Pour améliorer les résultats de traduction il est possible de :     \n",
        "\n",
        "* Augmenter la taille de la base de donnée d'entraînement.\n",
        "* Expérimenter avec différents paramètres tels que le nombre de couches et de neurones.\n",
        "* Utiliser une architecture alternatives telles que [les réseaux de neurones à attention](https://arxiv.org/pdf/1706.03762.pdf)"
      ],
      "metadata": {
        "id": "RQ6E8ioK1WFA"
      }
    }
  ]
}